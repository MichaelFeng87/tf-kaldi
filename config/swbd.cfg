[scheduler]
# minimum iterations for nnet training
min_iters = 5
# keep learning rate for this number of iterations
keep_lr_iters = 1
# halving learning rate by this factor if rel_impr not enough
halving_factor = 0.5
# start halving learning rate if rel_impr is small than this
start_halving_impr = 0.01
# end training if rel_impr is this small
end_halving_impr = 0.001
#number of passes over the entire database
max_iters = 20
#initial learning rate of the neural net
initial_learning_rate = 2.048

[feature]
#size of the left and right context window
context_width = 5
#size of the minibatch (#utterances)
batch_size = 256

[nnet]
#init nnet from kaldi file
#init_file = kaldi.nnet
#number of neurons in the hidden layers
hidden_units = 2048
#number of hidden layers
num_hidden_layers = 6
#nonlinearity used currently supported: relu, tanh, sigmoid
nonlin = sigmoid
#if you want to use dropout set to a value smaller than 1
#dropout = 1
#perform batch_normalization or not
batch_norm = False

[optimizer]
# optimizer type
name = sgd
# optimizer parameters
momentum = 0.9
