[scheduler]
# minimum iterations for nnet training
min_iters = 5
# keep learning rate for this number of iterations
keep_lr_iters = 1
# halving learning rate by this factor if rel_impr not enough
halving_factor = 0.5
# start halving learning rate if rel_impr is small than this
start_halving_impr = 0.01
# end training if rel_impr is this small
end_halving_impr = 0.001
#number of passes over the entire database
max_iters = 20
#initial learning rate of the neural net
initial_learning_rate = 0.4
#init nnet from kaldi file
#init_file = kaldi.nnet

[feature]
#size of the left and right context window
context_width = 5
#size of the minibatch (#utterances)
batch_size = 10
#maximum length of utterance (for lstm)
max_length = 40

[nnet]
#architecture of neural network, lstm or dnn
nnet_arch = lstm
#lstm type, LSTM or BLSTM
lstm_type = LSTM
#number of neurons in the hidden layers
num_cells = 256
#number of hidden layers
num_hidden_layers = 4
#if you want to use dropout set to a value smaller than 1
keep_in_prob = 0.8
#output of lstm keep prob
keep_out_prob = 0.8

[optimizer]
# optimizer type
op_type = sgd
# optimizer parameters
# momentum = 0.9

[general]
#nnet_proto = None
summary_dir = summary
#nnet_proto = nnet.proto
